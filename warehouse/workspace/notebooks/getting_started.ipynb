{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74243edb",
   "metadata": {},
   "source": [
    "# Getting Started with Spark & Iceberg\n",
    "\n",
    "**Important:** Spark session is **automatically created** when this notebook opens.\n",
    "\n",
    "The `spark` variable is ready to use immediately — no setup needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43be25",
   "metadata": {},
   "source": [
    "## Part 1: Spark Session (Already Running!)\n",
    "\n",
    "Your Spark session is created automatically by the PySpark Jupyter container.\n",
    "\n",
    "Just run the cell below to verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The spark session is automatically created in PySpark Jupyter\n",
    "# Let's verify it's working\n",
    "\n",
    "print(\"✓ Spark Session Active\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01785aa9",
   "metadata": {},
   "source": [
    "## Part 2: Verify Iceberg & Polaris Configuration\n",
    "\n",
    "Your Spark is configured to use Polaris as the catalog. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Spark configuration\n",
    "\n",
    "print(\"=== Catalog Configuration ===\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Polaris URI: {spark.conf.get('spark.sql.catalog.polaris.uri')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "print()\n",
    "print(\"✓ Everything configured correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866f1ca",
   "metadata": {},
   "source": [
    "## Part 3: Create Your First Namespace (Schema)\n",
    "\n",
    "A namespace is like a database or schema. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38937904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a namespace (schema) for your tables\n",
    "# This is just like CREATE DATABASE in traditional SQL\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS my_warehouse\")\n",
    "\n",
    "print(\"✓ Namespace created: my_warehouse\")\n",
    "\n",
    "# Verify it exists\n",
    "spark.sql(\"SHOW NAMESPACES IN polaris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158297b",
   "metadata": {},
   "source": [
    "## Part 4: Create Your First Iceberg Table\n",
    "\n",
    "Now create a simple table to store customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Iceberg table\n",
    "# USING ICEBERG makes it an Iceberg table (not Delta)\n",
    "# PARTITIONED BY helps with query performance\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS my_warehouse.customers (\n",
    "        customer_id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        country STRING,\n",
    "        created_at TIMESTAMP\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    PARTITIONED BY (CAST(created_at AS DATE))\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Table created: my_warehouse.customers\")\n",
    "\n",
    "# Show table schema\n",
    "spark.sql(\"DESCRIBE TABLE my_warehouse.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3053d9",
   "metadata": {},
   "source": [
    "## Part 5: Insert Sample Data\n",
    "\n",
    "Let's add some rows to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13aa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO my_warehouse.customers VALUES\n",
    "    (1, 'Alice Johnson', 'alice@example.com', 'USA', '2025-01-15 10:30:00'),\n",
    "    (2, 'Bob Smith', 'bob@example.com', 'Canada', '2025-01-16 14:20:00'),\n",
    "    (3, 'Charlie Brown', 'charlie@example.com', 'USA', '2025-01-17 09:15:00'),\n",
    "    (4, 'Diana Prince', 'diana@example.com', 'UK', '2025-01-18 11:45:00'),\n",
    "    (5, 'Eve Wilson', 'eve@example.com', 'USA', '2025-01-19 16:30:00')\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Inserted 5 rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a485b",
   "metadata": {},
   "source": [
    "## Part 6: Query Your Data\n",
    "\n",
    "The query pattern is simple: `schema.tablename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query - just use schema.tablename\n",
    "# No catalog prefix needed because polaris is the default catalog\n",
    "\n",
    "spark.sql(\"SELECT * FROM my_warehouse.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb30363",
   "metadata": {},
   "source": [
    "## Part 7: Filter Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data with WHERE clause\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, email, country \n",
    "    FROM my_warehouse.customers \n",
    "    WHERE country = 'USA'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c5698",
   "metadata": {},
   "source": [
    "## Part 8: Use DataFrame API (Alternative)\n",
    "\n",
    "You can also use the DataFrame API instead of SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table into a DataFrame\n",
    "df = spark.table(\"my_warehouse.customers\")\n",
    "\n",
    "# Filter and select columns\n",
    "df.filter(df.country == \"USA\").select(\"name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a5467",
   "metadata": {},
   "source": [
    "## Part 9: Verify Data Persists\n",
    "\n",
    "Your data is stored in MinIO and will persist even if you stop containers.\n",
    "Let's check the table metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table metadata and history\n",
    "\n",
    "print(\"=== Table Info ===\")\n",
    "spark.sql(\"SHOW TABLES IN my_warehouse\").show()\n",
    "\n",
    "print(\"\\n=== Row Count ===\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_rows FROM my_warehouse.customers\").show()\n",
    "\n",
    "print(\"\\n✓ Data is persisted in MinIO and PostgreSQL\")\n",
    "print(\"  - Even if you stop/restart containers, this data survives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7ec47",
   "metadata": {},
   "source": [
    "## Part 10: View Data in MinIO Console\n",
    "\n",
    "Your actual data files are stored in MinIO. To see them:\n",
    "\n",
    "1. Go to http://localhost:9001\n",
    "2. Login: `minioadmin` / `minioadmin`\n",
    "3. Navigate: `warehouse` → `polaris` → `my_warehouse` → `customers` → `data`\n",
    "4. You'll see `.parquet` files with your data!\n",
    "\n",
    "The metadata about these files is stored in PostgreSQL (via Polaris catalog)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bf195",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Spark Session**: Created automatically (already done!)\n",
    "\n",
    "✅ **Iceberg Table**: Created with `CREATE TABLE ... USING ICEBERG`\n",
    "\n",
    "✅ **Data Inserted**: Used SQL INSERT statement\n",
    "\n",
    "✅ **Data Queried**: Simple `schema.tablename` pattern\n",
    "\n",
    "✅ **Data Persists**: Stored in MinIO + PostgreSQL\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Create more tables** for your data\n",
    "2. **Join tables** together\n",
    "3. **Explore advanced features** like time travel in the example notebook\n",
    "4. **Add your own data** by creating CSV/Parquet files in `workspace/data/`\n",
    "\n",
    "### Key Takeaway:\n",
    "\n",
    "**You DON'T need to create a Spark session manually** — it's already created for you in PySpark Jupyter! Just use the `spark` variable directly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
